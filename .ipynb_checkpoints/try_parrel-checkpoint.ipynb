{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5495a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 Tensorforce Team. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "import gym\n",
    "from threading import Thread\n",
    "\n",
    "from tensorforce import Environment, Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfd9057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a181033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a7bbe031",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnvironment(Environment):\n",
    "\n",
    "    def __init__(self,number):\n",
    "        self.number=number\n",
    "        super().__init__()\n",
    "\n",
    "    def states(self):\n",
    "        return dict(type='float', shape=(8,))\n",
    "\n",
    "    def actions(self):\n",
    "        return dict(type='int', num_values=4)\n",
    "\n",
    "    # Optional: should only be defined if environment has a natural fixed\n",
    "    # maximum episode length; otherwise specify maximum number of training\n",
    "    # timesteps via Environment.create(..., max_episode_timesteps=???)\n",
    "    def max_episode_timesteps(self):\n",
    "        return super().max_episode_timesteps()\n",
    "\n",
    "    # Optional additional steps to close environment\n",
    "    def close(self):\n",
    "        super().close()\n",
    "\n",
    "    def reset(self):\n",
    "        state = np.random.random(size=(8,))\n",
    "        return state\n",
    "\n",
    "    def execute(self, actions):\n",
    "        next_state = np.random.random(size=(8,))\n",
    "        terminal = False  # Always False if no \"natural\" terminal state\n",
    "        time.sleep(0.0007)\n",
    "        reward = self.number\n",
    "        return next_state, terminal, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f6270ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No min_value bound specified for state.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "631b74f0d5c44b94ad3390405b3efbd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episodes:   0%|          | 0/100 [00:00, return=0.00, ts/ep=0, sec/ep=0.00, ms/ts=0.0, agent=0.0%]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    \"\"\"\n",
    "    Train agent on experience collected in parallel from 4 local CartPole environments.\n",
    "    Typical use case:\n",
    "        time for batched agent.act() ~ time for agent.act() > time for environment.execute()\n",
    "    \"\"\"\n",
    "agent = 'tensorforce/benchmarks/configs/ppo.json'\n",
    "runner = Runner(agent=agent, environment=dict(environment=CustomEnvironment,number=2), num_parallel=4,max_episode_timesteps=100)\n",
    "# Batch act/observe calls to agent, unless environment.is_vectorizable()\n",
    "# (otherwise essentially equivalent to single environment)\n",
    "runner.run(num_episodes=100, batch_agent_calls=True)\n",
    "runner.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ace10eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No min_value bound specified for state.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f20253b8c74601b3e3cfcfa48ad43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episodes:   0%|          | 0/100 [00:00, return=0.00, ts/ep=0, sec/ep=0.00, ms/ts=0.0, agent=0.0%]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    \"\"\"\n",
    "    Train agent on experience collected in parallel from 4 local CartPole environments.\n",
    "    Typical use case:\n",
    "        time for batched agent.act() ~ time for agent.act() > time for environment.execute()\n",
    "    \"\"\"\n",
    "agent = 'tensorforce/benchmarks/configs/ppo.json'\n",
    "runner = Runner(agent=agent, environment=dict(environment=CustomEnvironment,number=2),max_episode_timesteps=100)\n",
    "# Batch act/observe calls to agent, unless environment.is_vectorizable()\n",
    "# (otherwise essentially equivalent to single environment)\n",
    "runner.run(num_episodes=100)\n",
    "runner.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e05d2cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI-Gym environment specification\n",
    "environment1 = Environment.create(\n",
    "       environment='gym', level='CartPole-v1')\n",
    "\n",
    "environment2 = Environment.create(\n",
    "       environment='gym', level='CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23d477a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40474f91d016484b87c6436ce9720b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episodes:   0%|          | 0/100 [00:00, return=0.00, ts/ep=0, sec/ep=0.00, ms/ts=0.0, agent=0.0%]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    \"\"\"\n",
    "    Train agent on experience collected in parallel from 4 local CartPole environments.\n",
    "    Typical use case:\n",
    "        time for batched agent.act() ~ time for agent.act() > time for environment.execute()\n",
    "    \"\"\"\n",
    "agent = 'tensorforce/benchmarks/configs/ppo.json'\n",
    "environment = 'tensorforce/benchmarks/configs/cartpole.json'\n",
    "runner = Runner(agent=agent, environment=environment, num_parallel=2)\n",
    "# Batch act/observe calls to agent, unless environment.is_vectorizable()\n",
    "# (otherwise essentially equivalent to single environment)\n",
    "runner.run(num_episodes=100, batch_agent_calls=True)\n",
    "runner.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4a2417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_vectorized():\n",
    "    \"\"\"\n",
    "    Train agent on experience collected in parallel from one vectorized CartPole environment.\n",
    "    Typical use case:\n",
    "        time for vectorized environment < time for sequential execution\n",
    "    \"\"\"\n",
    "    agent = 'benchmarks/configs/ppo.json'\n",
    "    environment = 'custom_cartpole'\n",
    "    runner = Runner(agent=agent, environment=environment, max_episode_timesteps=500, num_parallel=4)\n",
    "    runner.run(num_episodes=100)\n",
    "    runner.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "833f5506",
   "metadata": {},
   "outputs": [
    {
     "ename": "TensorforceError",
     "evalue": "\n  File \"/home/olaf/Desktop/Semantic-Reasoning-in-Reinforcement-Learning/tensorforce/tensorforce/environments/environment.py\", line 683, in remote\n    env = Environment.create(\n  File \"/home/olaf/Desktop/Semantic-Reasoning-in-Reinforcement-Learning/tensorforce/tensorforce/environments/environment.py\", line 223, in create\n    return Environment.create(\n  File \"/home/olaf/Desktop/Semantic-Reasoning-in-Reinforcement-Learning/tensorforce/tensorforce/environments/environment.py\", line 204, in create\n    return Environment.create(\n  File \"/home/olaf/Desktop/Semantic-Reasoning-in-Reinforcement-Learning/tensorforce/tensorforce/environments/environment.py\", line 160, in create\n    environment = environment(**kwargs)\n  File \"/home/olaf/Desktop/Semantic-Reasoning-in-Reinforcement-Learning/tensorforce/tensorforce/environments/openai_gym.py\", line 170, in __init__\n    self.environment, self._max_episode_timesteps = self.__class__.create_level(\n  File \"/home/olaf/Desktop/Semantic-Reasoning-in-Reinforcement-Learning/tensorforce/tensorforce/environments/openai_gym.py\", line 70, in create_level\n    if level not in gym.envs.registry.env_specs:\n  File \"/home/olaf/anaconda3/envs/master/lib/python3.9/site-packages/gym/envs/registration.py\", line 409, in __contains__\n    namespace, name, version = parse_env_id(key)\n  File \"/home/olaf/anaconda3/envs/master/lib/python3.9/site-packages/gym/envs/registration.py\", line 71, in parse_env_id\n    raise error.Error(\n\n<class 'gym.error.Error'>: Malformed environment ID: benchmarks/configs/cartpole.json.(Currently all IDs must be of the form re.compile('^(?:(?P<namespace>[\\\\w:-]+)\\\\/)?(?:(?P<name>[\\\\w:.-]+?))(?:-v(?P<version>\\\\d+))?$').)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTensorforceError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbenchmarks/configs/ppo.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m environment \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbenchmarks/configs/cartpole.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 13\u001b[0m runner \u001b[38;5;241m=\u001b[39m \u001b[43mRunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_parallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmultiprocessing\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m runner\u001b[38;5;241m.\u001b[39mrun(num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_agent_calls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# optional: batch_agent_calls=True\u001b[39;00m\n\u001b[1;32m     15\u001b[0m runner\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Desktop/Semantic-Reasoning-in-Reinforcement-Learning/tensorforce/tensorforce/execution/runner.py:173\u001b[0m, in \u001b[0;36mRunner.__init__\u001b[0;34m(self, agent, environment, max_episode_timesteps, num_parallel, environments, evaluation, remote, blocking, host, port)\u001b[0m\n\u001b[1;32m    168\u001b[0m environment \u001b[38;5;241m=\u001b[39m Environment\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    169\u001b[0m     environment\u001b[38;5;241m=\u001b[39menvironments[\u001b[38;5;241m0\u001b[39m], max_episode_timesteps\u001b[38;5;241m=\u001b[39mmax_episode_timesteps,\n\u001b[1;32m    170\u001b[0m     remote\u001b[38;5;241m=\u001b[39mremote, blocking\u001b[38;5;241m=\u001b[39mblocking, host\u001b[38;5;241m=\u001b[39mhost[\u001b[38;5;241m0\u001b[39m], port\u001b[38;5;241m=\u001b[39mport[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    171\u001b[0m )\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_environment_remote \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(environment, RemoteEnvironment)\n\u001b[0;32m--> 173\u001b[0m states \u001b[38;5;241m=\u001b[39m \u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m actions \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39mactions()\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironments\u001b[38;5;241m.\u001b[39mappend(environment)\n",
      "File \u001b[0;32m~/Desktop/Semantic-Reasoning-in-Reinforcement-Learning/tensorforce/tensorforce/environments/environment.py:827\u001b[0m, in \u001b[0;36mRemoteEnvironment.states\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstates\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(function\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstates\u001b[39m\u001b[38;5;124m'\u001b[39m, kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m())\n\u001b[0;32m--> 827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstates\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Semantic-Reasoning-in-Reinforcement-Learning/tensorforce/tensorforce/environments/environment.py:787\u001b[0m, in \u001b[0;36mRemoteEnvironment.receive\u001b[0;34m(self, function)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39mproxy_close(connection\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection)\n\u001b[1;32m    786\u001b[0m etype, value, traceback \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m--> 787\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TensorforceError(message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(traceback), etype, value))\n",
      "\u001b[0;31mTensorforceError\u001b[0m: \n  File \"/home/olaf/Desktop/Semantic-Reasoning-in-Reinforcement-Learning/tensorforce/tensorforce/environments/environment.py\", line 683, in remote\n    env = Environment.create(\n  File \"/home/olaf/Desktop/Semantic-Reasoning-in-Reinforcement-Learning/tensorforce/tensorforce/environments/environment.py\", line 223, in create\n    return Environment.create(\n  File \"/home/olaf/Desktop/Semantic-Reasoning-in-Reinforcement-Learning/tensorforce/tensorforce/environments/environment.py\", line 204, in create\n    return Environment.create(\n  File \"/home/olaf/Desktop/Semantic-Reasoning-in-Reinforcement-Learning/tensorforce/tensorforce/environments/environment.py\", line 160, in create\n    environment = environment(**kwargs)\n  File \"/home/olaf/Desktop/Semantic-Reasoning-in-Reinforcement-Learning/tensorforce/tensorforce/environments/openai_gym.py\", line 170, in __init__\n    self.environment, self._max_episode_timesteps = self.__class__.create_level(\n  File \"/home/olaf/Desktop/Semantic-Reasoning-in-Reinforcement-Learning/tensorforce/tensorforce/environments/openai_gym.py\", line 70, in create_level\n    if level not in gym.envs.registry.env_specs:\n  File \"/home/olaf/anaconda3/envs/master/lib/python3.9/site-packages/gym/envs/registration.py\", line 409, in __contains__\n    namespace, name, version = parse_env_id(key)\n  File \"/home/olaf/anaconda3/envs/master/lib/python3.9/site-packages/gym/envs/registration.py\", line 71, in parse_env_id\n    raise error.Error(\n\n<class 'gym.error.Error'>: Malformed environment ID: benchmarks/configs/cartpole.json.(Currently all IDs must be of the form re.compile('^(?:(?P<namespace>[\\\\w:-]+)\\\\/)?(?:(?P<name>[\\\\w:.-]+?))(?:-v(?P<version>\\\\d+))?$').)`."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train agent on experience collected in parallel from 4 CartPole environments running in\n",
    "separate processes.\n",
    "Typical use case:\n",
    "    (a) time for batched agent.act() ~ time for agent.act()\n",
    "                    > time for environment.execute() + remote communication\n",
    "        --> batch_agent_calls = True\n",
    "    (b) time for environment.execute() > time for agent.act() + process communication\n",
    "        --> batch_agent_calls = False\n",
    "\"\"\"\n",
    "agent = 'benchmarks/configs/ppo.json'\n",
    "environment = 'benchmarks/configs/cartpole.json'\n",
    "runner = Runner(agent=agent, environment=environment, num_parallel=4, remote='multiprocessing')\n",
    "runner.run(num_episodes=100, batch_agent_calls=True)  # optional: batch_agent_calls=True\n",
    "runner.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f40b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def socket():\n",
    "    \"\"\"\n",
    "    Train agent on experience collected in parallel from 2 CartPole environments running on\n",
    "    another machine.\n",
    "    Typical use case: same as mode 2, but generally remote communication socket > process\n",
    "    Simulate remote environment, usually run on another machine via:\n",
    "        python run.py --environment gym --level CartPole-v1 --remote socket-server --port 65432\n",
    "    \"\"\"\n",
    "    agent = 'benchmarks/configs/ppo.json'\n",
    "    environment = 'benchmarks/configs/cartpole.json'\n",
    "\n",
    "    def server(port):\n",
    "        Environment.create(environment=environment, remote='socket-server', port=port)\n",
    "\n",
    "    server1 = Thread(target=server, kwargs=dict(port=65432))\n",
    "    server2 = Thread(target=server, kwargs=dict(port=65433))\n",
    "    server1.start()\n",
    "    server2.start()\n",
    "\n",
    "    runner = Runner(\n",
    "        agent=agent, num_parallel=2, remote='socket-client', host='127.0.0.1', port=65432\n",
    "    )\n",
    "    runner.run(num_episodes=100)  # optional: batch_agent_calls=True\n",
    "    runner.close()\n",
    "\n",
    "    server1.join()\n",
    "    server2.join()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
